
> 摘要：全民K歌直播间PK礼物是为拉动营收上线的一个较新项目。在其上线后的三周多时间里，其对产品的营收带来了一些积极的效果。本文从技术角度简述全民K歌直播间PK礼物这一项目的总体设计与后台实现。

###一、项目简述
pk礼物是直播间主播与粉丝互动的一种新的玩法。简单来说，在直播间中，主播侧可以发起两种礼物的pk，指明倒计时时长。在pk过程中，观众可以针对这两种礼物进行送礼。最终看哪种礼物胜出。有了这样一个入口，直播过程中就可以有多种玩法。比如：某礼物胜出主播唱首歌、另一种胜出跳支舞；pk胜出方topN主播送出线下礼品；连麦时的喜爱对比等。

###二、概要设计

####1. 总体设计
针对上述的需求，可以知道此项目涉及到多方消息的传递，对消息的实时性及容错性要求较高、对结果的最终一致性要求较高。对此，我们设计了服务器push与客户端pull相结合的消息传递方式，动态数据及定时器由server维护的统一管理方式。通过imsdk下发创建pk、结束pk及pk过程中的动态数据，并在imsdk概率性失效时通过客户端pull等方式来容错。
server提供创建pk、结束pk、获取pk过程中比对数据、获取最终结果、获取排行榜等接口。在主播创建pk后，server下发im消息到直播间中每个用户，将此pk加入到定时器中开始倒计时，并将此pk的相关信息存储到ckv中。直播间送礼行为发生后，server从hippo中拉取送礼的流水，判断是否pk的礼物之一，若是，将其累加到pk的送礼数据中，并更新排行榜，下发im消息至直播间。server判断某一pk倒计时结束后，同样写ckv，下发im消息，同时不再更新送礼的数据及排行榜。
server分为四类：管理pk场次的server、拉送礼流水计算排行的server、管理排行及统计信息的server、错误处理redo server。
下面从接口、存储、定时器及容错等方面展开介绍。

####2. 接口设计
接口涉及两个部分，一个是暴露给client的接口，即webapp的接口，一个是后端server提供的接口，即spp server的接口。此外，将im消息的设计也放在这部分介绍。

#####1) webapp

######a. 创建pk（createPk）
创建pk时需要客户端带上showId，pk的两个giftId，每种礼物对应的文案（如：投它，我唱首浮夸），以及pk的默认时长。server收到消息后生成pkId下发，并带上interval字段，指示client pull的间隔时长。

######b. 结束pk（destoryPk）
结束pk有两种方式，一种是client创建后不用关心什么时候关闭，等着server倒计时结束后自动下发的im消息即可。第二种是client主播测在pk还没有结束时手动触发关闭。在后一种情况下client需要将showId与pkId传上来，server回复中payload为空（但出错时会在包头有错误码）。

######c. 获取pk过程中比对数据（getStat）
一旦主播侧创建成功，或者观众侧收到server下发的创建pk的im消息，client会按照interval指示的时长定期pull数据。这时调用的就是此接口。正常情况下pull时只要带上showId即可，但有一个补充bool字段needExtraInfo，其作为一个容错的字段，只有在观众侧没有收到创建pk的im消息但收到送礼时的im消息后需要置位后带上来。这点会在后面的容错章节详细说明。此不赘述。

######d. 获取最终结果（getResultStat）
pk结束后，client拉取最终pk结果展示，需调用此接口。只需上传pkId即可。server返回两种礼物的送礼总数、对战总时长、当前用户的额送礼总数，以及一个送礼排行榜，这个榜只含有胜出一方的前5位，若打平，需要综合排名前5位（即不区分送的是哪个礼物）。同时有一个容错的字段，标识pk是否真的结束了，防止客户端倒计时过快导致较早拉取了非最终的结果。这点也会在容错章节详细说明。

######e. 获取排行榜（getRank）
pk进行中时，用户点击pk比对数据的小图标可以呼起礼物对战的排行。这里会调用此接口。需要带上pkId、giftId以及一个index字段，index指示从排行榜第几位开始下发，目前设置一次下发20条榜单信息。

#####2) server
除包含上述接口外，还包括：

######a. 获取pkInfo（getPkInfo）
当拉取送礼流水的server从hippo中拉出了送礼数据后，需要判断是否要计算到排行榜中，这时候需要请求获取一次pkInfo，根据giftId及送礼的时间戳判断是否要计算进排行榜中。因为hippo中拉出的流水只有showId，没有pkId信息，所以此接口返回的是此showId下最后一场的pkInfo。

#####3) im消息

######a. 创建pk消息（imCreatePk）
主播侧创建后server需要给直播间所有观众推送创建pk的消息，由于观众侧很有可能没有拉过礼物面板，所以此消息除了pkId、pk时长、pull间隔时长、giftId，还需要带上giftName、giftLogo。

######b. 结束pk（imDestroyPk）
结束时的im消息只要带上pkId即可。

######c. pk过程比对数据（imPkStat）
下发pkId、giftId及送礼总数，此外还有个timeLeft字段指示还有多久pk结束，防止client倒计时不准确。

####3. 存储设计
存储均使用ckv存储，主要有以下四类。

#####1) pkInfo
pk的管理信息。key为showId。将一场直播下面的所有pk放在一个vector中存储。其中每条存放pkId、status（进行中|销毁）、giftId、giftDesc（创建时的文案）、giftName、giftLogo、createTime、destroyTime、创建时长等。

#####2) statInfo
pk的比对数据。key为pkId。存储礼物对应的送礼个数和送礼总KB数。

#####3) rankInfo
pk的排行榜数据。key为pkId_giftId。存储每场pk的每个礼物下的排行榜数据。

#####4) userInfo
每个user的送礼数据。key为pkId_uId_giftId。

####4. 定时器设计
由于pk礼物需要有定时结束的功能，所以在服务端维护一个定时器就必不可少。一般来说，定时器有两种常用高效的实现方式。一种是小根堆，另一种是时间轮。这里我们采用时间轮的方式来管理。时间轮只提供push和pull两个接口。push用来插入元素，并设置超时；而调用pull会返回当前超时的元素并将其从时间轮中删除。每个服务进程维护自己的时间轮，pk由哪个server创建，则此pk由哪个负责销毁。新发起的pk，将pkId插入，设置超时时间为主播预设时长。从轮中pull出的pkId，首先要去看ckv中pkInfo是否已经是已销毁状态，若不是，将其置为销毁状态后再下发im的结束pk的消息。这里操作的顺序尤为重要，主要是为了容错之用，一是主播可能手动关了pk，二是某server宕机后将失效的元素重新加入时间轮时无法保证唯一性，为防止im消息重复下发而做的保护。这块会在容错章节做详细说明。

####5. 容错设计
上述章节都是一些总体的设计，描述的多是正常流程的情况。而服务器业务逻辑的设计对异常情况的考虑必不可少。简单罗列包括消息丢失、消息重复、定时器失效等情况。下面一一介绍。

#####1) 消息丢失

######a. imCreatePk
这里可能又要分为三种情况：调用imsdk接口失败，imsdk调用成功但部分客户端没有收到，imsdk调用成功但所有客户端都没有收到。
第一种情况会回复创建失败的给主播侧，客户端会有重试逻辑。
第二种情况，收到im消息的观众可能拉起了pk浮层，玩的热火朝天，但部分观众一脸懵逼不知道发生了什么。由于每次有pk送礼行为产生后均会广播下发imPkStat消息，客户端收到这个im消息时，将其中的pkId与本机保存的本场直播所有pkId列表比对，就知道是否是一场新的直播开启了而本机没有收到创建pk的im消息。这种情况下，由于本机没有imCreatePk中一些giftName和giftLogo的信息，客户端会立即发出一个needExtraInfo置了位的getStat请求，server就知道客户端没收到imCreatePk的消息，从而附带下发一些必要的信息以供客户端拉起pk浮层。
而第三种情况下，只有主播拉起了pk浮层，所有观众都没有拉起，也没有任何人送礼。第一，从技术层面，imsdk是基于可靠的tcp传输，应该除了传输层的确认外，应用层也会做消息的确认，出现所有客户端都下发失败的概率极小；第二，如果真的出现，可能主播与观众的互动交流中会发现这个情况然后手动关闭此场pk后重新开启。所以我们这里不处理这种情况。

######b. imDestroyPk
其一，客户端会自己维护一个倒计时定时器，时间到了后会自动关闭，并不完全依赖imDestroyPk。其二，如果是提前主播手动关闭或者是客户端定时器失效，那么当客户端定期发送请求getStat时，server会告知客户端已经关闭了，告知的方式是将回复中的pkId字段置空。

######c. imPkStat
之所以在pk过程中动态比对数据采用推拉结合的方式，主要就是为了防止imPkStat消息丢失或者延迟严重。所以这种情况是通过客户端定时的getStat来容错。

######d. 客户端请求的回复
createPk、destroyPk、getResultStat、getRank在客户端都是有重试逻辑。而getStat不需要重试，因为pull失败带来的影响最多是客户端pk比对数据更新没那么及时，影响不大。

#####2) 消息重复
对于幂等的接口，重复没有影响。比如getStat，getResultStat，getRank。对于不幂等的接口，需要分开讨论。

######a. createPk
后来的createPk消息，根据showId判断是否已经有正在进行的pk，如果有，返回对于的errorcode，客户端来请求之前的场次信息重新拉起pk浮层即可。

######b. destroyPk
判断是否已经将ckv中此场pk的销毁状态位置位，若否，置位。但不管前面判断是什么结果，都是返回成功给客户端。


#####3) 定时器失效

######a. 服务器定时器失效
由于服务器的定时器是由每一个进程单独维护的。某一场pk只会存在于某一个进程的定时器中，没有备份。所以当某一个进程down掉之后，其所维护定时器中所有的元素都失效了。带来的后果就是pk无法自动结束。下一场pk无法创建。这里通过客户端pull时判断一下pk是否到期而没有结束。如果是，则将其重新插入定时器中，下一个定时器周期会将其取出执行关闭流程。这里就有可能多个server接收同时收到客户端的pull，同时判断出需要重新将此pkId插入定时器中，就会pkId出现在多个server中的情况。这是也就需要定时器时间到了之后，即使有多个相同的pkId也只发送一次imDestroyPk消息（多次发送也没有关系，但是为了防止资源浪费，消息泛滥，还是有必要加上防重复的逻辑）。具体做法就是先判断pkInfo中的关闭标志位。这个在上面的定时器设计中也有说明。

######b. 客户端定时器失效
客户端定时器可能跑快了或者跑慢了。这时需要对其进行纠正。这里简单的采用客户端pull时将服务器的定时剩余时间下发即可。imPkStat的消息也会带上这个剩余时间字段。但是由于im消息延迟可能比较严重，所以默认情况下忽略这个时间，只有在imPkStat的剩余时间比client维护的剩余时间要短时才进行更新。
client最后还是有概率在最后一个pull更新了剩余时间之后跑快结束了pk，这时client来getResultStat，回复未结束进行容错。
如果是client最后一次pull更新了剩余时间之后跑慢了，继续来pull，那么会回复空的pkId告知其已经关闭。

#####4) 一些server处理的错误
有些错误是直接返回错误码就可以，客户端针对不同错误码进行不同逻辑处理。有些错误则没有这样的机会，比如拉hippo流水后的一系列处理，它们必须保证最终的成功。这时就需要redo server，在发生错误时写入错误日志并记录错误点，redo server则负责不断读取这些错误日志，并从错误点开始继续处理。如果还是错误，则要继续循环处理，直至成功。目前，K歌这边的redo都是将错误记录到文件系统的文件中。我这里也是采用的前人的做法。


###三、线上问题及处理
pk礼物功能以及上线三周有余，这个过程其访问量逐渐提升，也暴露了一些问题。

#####1) 创建pk后无法送礼
我将一场直播下的所有场次的pkInfo均放在同一个showId的key下存到ckv中。由于之前以为jce不光是对传输结构体进行编码还会压缩。想当然的以为不会有正常字符出现在encode之后的jce结构体中。又由于我对jce的理解是仅供传输之用，不必要的东西不应该放在里面。所以当需要将多个pkInfo存到同一个key下时，我首先想到的不是在jce中定义一个vector，而是将多个encode之后的pkInfo连起来放一起。采用的方案是用';'进行分割。我自测也没问题，测试也没测出来问题。但是线上跑了一段时间后，有次发现怎么可以创建，但送礼送不进去。看了后台日志后才知道，主播在创建pk时指定的文案中有';'字符。而这个文案我是放到pkInfo中的。导致取ckv时分隔出错。改成在jce中定义vct后就没有问题了。好在这个问题在上线后不久发现，当时请求量很少，没有造成较大影响（每分钟约300个getStat，按照间隔是10s计算，大概有50个客户端当时在正在pk的直播间里，出错的只是其中的一间）。

#####2) 客户端getStat逻辑错误
上面的问题出现并改正后，发现后台还是有请求失败，持续了一整夜。而且请求带的参数一样，直播结束后还在不断请求。定位后发现客户端没有对getStat回复的pkId为空进行处理（应该认为没有pk了，关闭请求）。告知后得到修正。

#####3) getResultStat返回pk总时长多了1s
这是上4.0版本时，测试同学在测试环境测出来的。分析后认为原因多方面：1.测试环境负载很高，进程切换代价较大，较为耗时；2.定时器中取出后处理的耗时。处理办法是在最后做一道把关，返回的总时长不能超过创建时指定的时长即可。

#####4) getResultStat周末晚间有时报警
定位后发现，pk结束的时候，由于所有观众会同时来请求结果数据，负载会一下增大几十倍。这种情况在大V的直播间尤为明显。也就是说，pk server的访问毛刺现象比较严重。对于这种情况，目前只能通过增大服务进程数来解决。




####后记：
1. 以前的工作是写较为底层的socket，类似wns客户端，对业务逻辑涉足较少，不太懂写server需要注意的问题。通过这一次完整的项目经历，期间较完善全面的了解了各个组件工具的用法，了解了需要考虑的方方面面。自我感觉受益匪浅。特别感谢奎哥、龙哥、jerry的不吝赐教。
2. 回想一下，感觉有些地方有改进的空间，比如redo是不是可以写到hippo里面减少磁盘io、server应该全面换成微线程增大吞吐、spp是不是可以做成动态扩容等。


